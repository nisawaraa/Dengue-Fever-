{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "KukFtL4GiDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML"
      ],
      "metadata": {
        "id": "spa5RtdlyGwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_final_clean = pd.read_excel(\"/content/df_final_clean.xlsx\")\n",
        "df_final_clean_encoded = df_final_clean.copy()\n",
        "\n",
        "# ‚úÖ 3. ‡πÅ‡∏õ‡∏•‡∏á categorical ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
        "for col in df_final_clean_encoded.select_dtypes(include=['object', 'category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df_final_clean_encoded[col] = le.fit_transform(df_final_clean_encoded[col]).astype(int)\n",
        "\n",
        "df_final_clean_encoded = df_final_clean_encoded.drop(columns=['date'])\n",
        "df_final_clean_encoded"
      ],
      "metadata": {
        "id": "U0tCjJhSwbNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_clean_encoded.corr()['cases']"
      ],
      "metadata": {
        "id": "VroHPVHXwov8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î X ‡πÅ‡∏•‡∏∞ y\n",
        "X = df_final_clean_encoded.drop(columns=['‡πÄ‡∏û‡∏®','‡πÄ‡∏î‡∏∑‡∏≠‡∏ô','‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)','‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™','humid_15d_avg'])\n",
        "y = df_final_clean['cases']\n",
        "\n",
        "# ‚úÖ 5. ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Gradient Boosting\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ‚úÖ 7. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ 8. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# ‚úÖ 9. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
        "print(f\"‚úÖ R¬≤: {r2:.3f}\")\n",
        "\n",
        "# ‚úÖ 10. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ vs ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values, label='Actual Cases', color='red')\n",
        "plt.plot(y_pred, label='Predicted Cases', color='blue')\n",
        "plt.title('Predicted vs Actual Cases')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vusT-Fxwh-J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# ‚úÖ 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î X ‡πÅ‡∏•‡∏∞ y\n",
        "X = df_final_clean_encoded.drop(columns=['‡πÄ‡∏û‡∏®','‡πÄ‡∏î‡∏∑‡∏≠‡∏ô','‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)','‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™','humid_15d_avg'])\n",
        "y = df_final_clean['cases']\n",
        "\n",
        "# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ‚úÖ ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"‚úÖ XGBoost RMSE: {rmse:.2f}\")\n",
        "print(f\"‚úÖ XGBoost R¬≤: {r2:.3f}\")\n",
        "\n",
        "# ‚úÖ ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values, label='Actual Cases', color='red')\n",
        "plt.plot(y_pred, label='Predicted Cases (XGBoost)', color='green')\n",
        "plt.title('Predicted vs Actual Cases (XGBoost)')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BYcvRW4tgg9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ‚úÖ 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î X ‡πÅ‡∏•‡∏∞ y (‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å)\n",
        "X = df_final_clean_encoded.drop(columns=['‡πÄ‡∏û‡∏®','‡πÄ‡∏î‡∏∑‡∏≠‡∏ô','‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)','‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™','humid_15d_avg'])\n",
        "y = df_final_clean['cases']\n",
        "\n",
        "# ‚úÖ 2. ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î train/test (‡πÉ‡∏´‡∏°‡πà)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ 4. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ‚úÖ 5. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"‚úÖ XGBoost RMSE: {rmse:.2f}\")\n",
        "print(f\"‚úÖ XGBoost R¬≤: {r2:.3f}\")\n",
        "\n",
        "# ‚úÖ 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Cross-validation R¬≤\n",
        "cv_r2_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')\n",
        "print(f\"üìä Cross-validated R¬≤ (mean): {cv_r2_scores.mean():.3f}\")\n",
        "print(f\"üìä Cross-validated R¬≤ (std): {cv_r2_scores.std():.3f}\")\n",
        "\n",
        "# ‚úÖ 7. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ vs ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values, label='Actual Cases', color='red')\n",
        "plt.plot(y_pred, label='Predicted Cases (XGBoost)', color='green')\n",
        "plt.title('Predicted vs Actual Cases (XGBoost)')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DEf6O_zkrlEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "X = df_final_clean_encoded.drop(columns=['‡πÄ‡∏û‡∏®','‡πÄ‡∏î‡∏∑‡∏≠‡∏ô','‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)','‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™','humid_15d_avg'])\n",
        "y = df_final_clean['cases']\n",
        "\n",
        "# ‚úÖ 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ‚úÖ 7. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ 8. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# ‚úÖ 9. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"‚úÖ Random Forest RMSE: {rmse:.2f}\")\n",
        "print(f\"‚úÖ Random Forest R¬≤: {r2:.3f}\")\n",
        "\n",
        "# ‚úÖ 10. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ vs ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test.values, label='Actual Cases', color='red')\n",
        "plt.plot(y_pred, label='Predicted Cases (RF)', color='orange')\n",
        "plt.title('Random Forest: Predicted vs Actual Cases')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kxkYE3bsgg_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J47P0TVxhhtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df = pd.read_excel(\"/content/df_final_clean.xlsx\")\n",
        "\n",
        "# Label Encoding\n",
        "df_ml = df.copy()\n",
        "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_ml[col] = le.fit_transform(df_ml[col].astype(str))\n",
        "\n",
        "# Features & Target\n",
        "X_cols = ['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "X = df_ml[X_cols]\n",
        "y = df_ml['cases']\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === VIF ===\n",
        "X_vif = df_ml[['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)']]\n",
        "X_vif_const = sm.add_constant(X_vif)\n",
        "vif_df = pd.DataFrame()\n",
        "vif_df[\"Feature\"] = X_vif_const.columns\n",
        "vif_df[\"VIF\"] = [variance_inflation_factor(X_vif_const.values, i) for i in range(X_vif_const.shape[1])]\n",
        "print(\"üìä VIF:\")\n",
        "print(vif_df)\n",
        "\n",
        "# === Preprocessing ===\n",
        "X_cat = ['‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "X_num = ['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)']\n",
        "preproc = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), X_num),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), X_cat)\n",
        "])\n",
        "\n",
        "# === ML Models ===\n",
        "def run_model(model, name):\n",
        "    pipe = Pipeline([(\"prep\", preproc), (\"model\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = np.clip(pipe.predict(X_test), 0, None)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    print(f\"{name} - RMSE: {rmse:.3f}, R¬≤: {r2:.3f}\")\n",
        "    return preds, y_test - preds\n",
        "\n",
        "pred_lasso, resid_lasso = run_model(SGDRegressor(loss='squared_error', penalty='l1', alpha=1e-4, max_iter=2000), \"Lasso\")\n",
        "pred_ridge, resid_ridge = run_model(SGDRegressor(loss='squared_error', penalty='l2', alpha=1e-4, max_iter=2000), \"Ridge\")\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=3)\n",
        "gbr.fit(X_train, y_train)\n",
        "pred_gbr = gbr.predict(X_test)\n",
        "resid_gbr = y_test - pred_gbr\n",
        "print(\"Gradient Boosting - RMSE:\", np.sqrt(mean_squared_error(y_test, pred_gbr)), \"R¬≤:\", r2_score(y_test, pred_gbr))\n",
        "\n",
        "xgbr = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=3)\n",
        "xgbr.fit(X_train, y_train)\n",
        "pred_xgb = xgbr.predict(X_test)\n",
        "resid_xgb = y_test - pred_xgb\n",
        "print(\"XGBoost - RMSE:\", np.sqrt(mean_squared_error(y_test, pred_xgb)), \"R¬≤:\", r2_score(y_test, pred_xgb))\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "pred_rf = rf.predict(X_test)\n",
        "resid_rf = y_test - pred_rf\n",
        "print(\"Random Forest - RMSE:\", np.sqrt(mean_squared_error(y_test, pred_rf)), \"R¬≤:\", r2_score(y_test, pred_rf))\n",
        "\n",
        "# === Feature Importance ===\n",
        "xgb_importance = pd.Series(xgbr.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "rf_importance = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "# === Residual Plot ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(pred_xgb, resid_xgb, alpha=0.4)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals vs Predicted (XGBoost)\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residuals\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(pred_rf, resid_rf, alpha=0.4)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals vs Predicted (Random Forest)\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residuals\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ===\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Lasso\", \"Ridge\", \"Gradient Boosting\", \"XGBoost\", \"Random Forest\"],\n",
        "    \"RMSE\": [\n",
        "        np.sqrt(mean_squared_error(y_test, pred_lasso)),\n",
        "        np.sqrt(mean_squared_error(y_test, pred_ridge)),\n",
        "        np.sqrt(mean_squared_error(y_test, pred_gbr)),\n",
        "        np.sqrt(mean_squared_error(y_test, pred_xgb)),\n",
        "        np.sqrt(mean_squared_error(y_test, pred_rf)),\n",
        "    ],\n",
        "    \"R¬≤\": [\n",
        "        r2_score(y_test, pred_lasso),\n",
        "        r2_score(y_test, pred_ridge),\n",
        "        r2_score(y_test, pred_gbr),\n",
        "        r2_score(y_test, pred_xgb),\n",
        "        r2_score(y_test, pred_rf),\n",
        "    ]\n",
        "})\n",
        "print(\"\\nüìä ‡∏ú‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ML:\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "c-97gnLJrREa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"
      ],
      "metadata": {
        "id": "0OpdA6xssBYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LassoCV, RidgeCV\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_excel(\"/content/df_final_clean.xlsx\")\n",
        "\n",
        "# === Label Encoding ===\n",
        "df_ml = df.copy()\n",
        "cat_cols = df_ml.select_dtypes(include=['object', 'category']).columns\n",
        "for col in cat_cols:\n",
        "    df_ml[col] = LabelEncoder().fit_transform(df_ml[col].astype(str))\n",
        "\n",
        "# === Define X, y ===\n",
        "X_cols = ['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "X = df_ml[X_cols]\n",
        "y = df_ml['cases']\n",
        "\n",
        "# === Split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Stepwise Selection Function ===\n",
        "def stepwise_selection(X, y,\n",
        "                       initial_list=[],\n",
        "                       threshold_in=0.01,\n",
        "                       threshold_out=0.05,\n",
        "                       direction='both'):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "        excluded = list(set(X.columns) - set(included))\n",
        "        new_pval = pd.Series(index=excluded, dtype=float)\n",
        "        for new_col in excluded:\n",
        "            try:\n",
        "                model = sm.GLM(y, sm.add_constant(X[included + [new_col]]), family=sm.families.NegativeBinomial()).fit()\n",
        "                new_pval[new_col] = model.pvalues[new_col]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if direction in ['forward', 'both'] and not new_pval.empty:\n",
        "            best_pval = new_pval.min()\n",
        "            if best_pval < threshold_in:\n",
        "                best_feature = new_pval.idxmin()\n",
        "                included.append(best_feature)\n",
        "                changed = True\n",
        "\n",
        "        if direction in ['backward', 'both'] and included:\n",
        "            model = sm.GLM(y, sm.add_constant(X[included]), family=sm.families.NegativeBinomial()).fit()\n",
        "            pvalues = model.pvalues.iloc[1:]\n",
        "            worst_pval = pvalues.max()\n",
        "            if worst_pval > threshold_out:\n",
        "                worst_feature = pvalues.idxmax()\n",
        "                included.remove(worst_feature)\n",
        "                changed = True\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "    return included\n",
        "\n",
        "# === Evaluate NB Model ===\n",
        "def evaluate_nb_model(X_train, X_test, y_train, y_test, selected_vars, model_name=\"\"):\n",
        "    X_train_sel = sm.add_constant(X_train[selected_vars])\n",
        "    X_test_sel = sm.add_constant(X_test[selected_vars])\n",
        "\n",
        "    model = sm.GLM(y_train, X_train_sel, family=sm.families.NegativeBinomial()).fit()\n",
        "    y_pred = model.predict(X_test_sel)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    residuals = y_test - y_pred\n",
        "    pearson_chi2 = np.sum(model.resid_pearson ** 2)\n",
        "    overdispersion = pearson_chi2 / model.df_resid\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Variables': selected_vars,\n",
        "        'RMSE': rmse,\n",
        "        'R2': r2,\n",
        "        'Overdispersion': overdispersion,\n",
        "        'Residuals': residuals,\n",
        "        'ModelObject': model\n",
        "    }\n",
        "\n",
        "# === VIF Function ===\n",
        "def check_vif(X_vars):\n",
        "    X_const = sm.add_constant(X_vars)\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"Variable\"] = X_const.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "    return vif\n",
        "\n",
        "# === Run Variable Selection Methods ===\n",
        "results = []\n",
        "\n",
        "# Forward Selection\n",
        "fwd_vars = stepwise_selection(X_train, y_train, direction='forward')\n",
        "results.append(evaluate_nb_model(X_train, X_test, y_train, y_test, fwd_vars, \"Forward\"))\n",
        "\n",
        "# Backward Elimination\n",
        "bwd_vars = stepwise_selection(X_train, y_train, direction='backward', initial_list=list(X_train.columns))\n",
        "results.append(evaluate_nb_model(X_train, X_test, y_train, y_test, bwd_vars, \"Backward\"))\n",
        "\n",
        "# Stepwise Selection\n",
        "stp_vars = stepwise_selection(X_train, y_train, direction='both')\n",
        "results.append(evaluate_nb_model(X_train, X_test, y_train, y_test, stp_vars, \"Stepwise\"))\n",
        "\n",
        "# Lasso Selection\n",
        "X_scaled = StandardScaler().fit_transform(X_train)\n",
        "lasso = LassoCV(cv=5).fit(X_scaled, y_train)\n",
        "lasso_vars = X.columns[lasso.coef_ != 0].tolist()\n",
        "results.append(evaluate_nb_model(X_train, X_test, y_train, y_test, lasso_vars, \"Lasso\"))\n",
        "\n",
        "# Ridge Selection (top 8)\n",
        "ridge = RidgeCV(cv=5).fit(X_scaled, y_train)\n",
        "ridge_coef = pd.Series(np.abs(ridge.coef_), index=X.columns)\n",
        "ridge_vars = ridge_coef.sort_values(ascending=False).head(8).index.tolist()\n",
        "results.append(evaluate_nb_model(X_train, X_test, y_train, y_test, ridge_vars, \"Ridge\"))\n",
        "\n",
        "# === Summary Table ===\n",
        "summary_df = pd.DataFrame([{\n",
        "    \"Model\": r['Model'],\n",
        "    \"Num Vars\": len(r['Variables']),\n",
        "    \"RMSE\": round(r['RMSE'], 3),\n",
        "    \"R¬≤\": round(r['R2'], 3),\n",
        "    \"Overdispersion\": round(r['Overdispersion'], 3)\n",
        "} for r in results])\n",
        "\n",
        "print(\"üìä Summary of NB Regression Models:\")\n",
        "print(summary_df)\n",
        "\n",
        "# === VIF ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏à‡∏≤‡∏Å R¬≤ ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ===\n",
        "best_model = max(results, key=lambda x: x['R2'])\n",
        "print(f\"\\n‚úÖ Best Model: {best_model['Model']}\")\n",
        "print(\"üîé Variables Used:\", best_model['Variables'])\n",
        "print(\"\\nüìå VIF ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:\")\n",
        "print(check_vif(X_train[best_model['Variables']]))\n"
      ],
      "metadata": {
        "id": "4nxGvKxW4cr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô\n",
        "all_vars = ['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "\n",
        "# ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ VIF ‡∏™‡∏π‡∏á‡∏≠‡∏≠‡∏Å: '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™' ‡πÅ‡∏•‡∏∞ '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'\n",
        "reduced_vars = [var for var in all_vars if var not in ['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']]\n",
        "\n",
        "# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà\n",
        "reduced_result_no_quarter_month = evaluate_nb_model(\n",
        "    X_train, X_test, y_train, y_test,\n",
        "    reduced_vars,\n",
        "    model_name=\"Ridge_Reduced_No_Quarter_Month\"\n",
        ")\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "print(\"üìâ Results after removing '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™' and '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô':\")\n",
        "print(f\"Model: {reduced_result_no_quarter_month['Model']}\")\n",
        "print(f\"Variables: {reduced_result_no_quarter_month['Variables']}\")\n",
        "print(f\"RMSE: {reduced_result_no_quarter_month['RMSE']:.3f}\")\n",
        "print(f\"R¬≤: {reduced_result_no_quarter_month['R2']:.3f}\")\n",
        "print(f\"Overdispersion: {reduced_result_no_quarter_month['Overdispersion']:.3f}\")\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö VIF ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠\n",
        "print(\"\\nüìä VIF ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà:\")\n",
        "print(check_vif(X_train[reduced_vars]))\n"
      ],
      "metadata": {
        "id": "JeBZanXb34yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Forward Selection\n",
        "forward_vars = ['‡∏õ‡∏µ', '‡∏ï‡∏≥‡∏ö‡∏•']  # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ 2 ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ\n",
        "forward_result = evaluate_nb_model(X_train, X_test, y_train, y_test, forward_vars, model_name=\"NB_Forward\")\n",
        "\n",
        "# 2. Backward Selection\n",
        "backward_vars = ['‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ï‡∏≥‡∏ö‡∏•']  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏à‡∏≤‡∏Å backward\n",
        "backward_result = evaluate_nb_model(X_train, X_test, y_train, y_test, backward_vars, model_name=\"NB_Backward\")\n",
        "\n",
        "# 3. Stepwise\n",
        "stepwise_vars = ['‡∏õ‡∏µ', '‡∏ï‡∏≥‡∏ö‡∏•']  # ‡∏ï‡∏≤‡∏°‡∏ú‡∏• stepwise\n",
        "stepwise_result = evaluate_nb_model(X_train, X_test, y_train, y_test, stepwise_vars, model_name=\"NB_Stepwise\")\n",
        "\n",
        "# 4. Lasso\n",
        "lasso_vars = ['‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', 'humid_15d_avg']  # ‡∏à‡∏≤‡∏Å‡∏ú‡∏• Lasso\n",
        "lasso_result = evaluate_nb_model(X_train, X_test, y_train, y_test, lasso_vars, model_name=\"NB_Lasso\")\n",
        "\n",
        "# 5. Ridge (‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏î VIF ‡∏™‡∏π‡∏á)\n",
        "ridge_vars = ['‡∏õ‡∏µ', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡∏ï‡∏≥‡∏ö‡∏•', 'humid_15d_avg', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û']  # ‡∏ï‡∏±‡∏î‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏Å‡∏±‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô\n",
        "ridge_result = evaluate_nb_model(X_train, X_test, y_train, y_test, ridge_vars, model_name=\"Ridge_Reduced_No_Quarter_Month\")\n"
      ],
      "metadata": {
        "id": "3GxvA3XRv6OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = [\n",
        "    forward_result,\n",
        "    backward_result,\n",
        "    stepwise_result,\n",
        "    lasso_result,\n",
        "    ridge_result\n",
        "]\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
        "for res in results:\n",
        "    print(f\"üî∏ Model: {res['Model']}\")\n",
        "    print(f\"Variables: {res['Variables']}\")\n",
        "    print(f\"RMSE: {res['RMSE']:.3f}, R¬≤: {res['R2']:.3f}, Overdispersion: {res['Overdispersion']:.3f}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "zowL1xUTwQv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ AIC ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ModelObject (‡∏à‡∏≤‡∏Å statsmodels)\n",
        "for res in results:\n",
        "    try:\n",
        "        res['AIC'] = res['ModelObject'].aic\n",
        "    except:\n",
        "        res['AIC'] = None\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': r['Model'],\n",
        "    'Num_Variables': len(r['Variables']),\n",
        "    'RMSE': round(r['RMSE'], 3),\n",
        "    'R¬≤': round(r['R2'], 3),\n",
        "    'Overdispersion': round(r['Overdispersion'], 3),\n",
        "    'AIC': round(r['AIC'], 2) if r['AIC'] is not None else 'N/A'\n",
        "} for r in results])\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
        "print(\"\\nüìä Summary Comparison Table:\")\n",
        "print(summary_df.sort_values(by='RMSE'))\n"
      ],
      "metadata": {
        "id": "yw6bbO4gD7Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features & target\n",
        "X_train_ml = X_train[ridge_vars]  # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô NB model\n",
        "X_test_ml = X_test[ridge_vars]\n",
        "\n",
        "# 2. Random Forest\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train_ml, y_train)\n",
        "y_pred_rf = rf.predict(X_test_ml)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "# 3. XGBoost\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "xgb.fit(X_train_ml, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test_ml)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "0kPOy671wUU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä RMSE Comparison\")\n",
        "print(f\"NB Model (Ridge Reduced): {ridge_result['RMSE']:.3f}\")\n",
        "print(f\"Random Forest          : {rmse_rf:.3f}\")\n",
        "print(f\"XGBoost                : {rmse_xgb:.3f}\")\n"
      ],
      "metadata": {
        "id": "IlgcQEU5BgQf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}