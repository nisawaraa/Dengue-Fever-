{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Google Colab ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤ patient_data.xlsx\n",
        "patients = pd.read_excel('DHFcc12.xlsx')  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n"
      ],
      "metadata": {
        "id": "QqR6w-nVnHGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ó‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "xls = pd.ExcelFile('DHFcc12.xlsx')  # <-- ‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á\n",
        "\n",
        "# ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "sheet_names = xls.sheet_names\n",
        "\n",
        "# ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ó‡πÄ‡∏Ç‡πâ‡∏≤ DataFrame ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "df_all = pd.concat([xls.parse(sheet) for sheet in sheet_names], ignore_index=True)\n",
        "\n",
        "# ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "df_all.head()\n"
      ],
      "metadata": {
        "id": "sb_BTjS3lbHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patients = df_all  # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏î‡∏¥‡∏°\n"
      ],
      "metadata": {
        "id": "fr4YoP8tnkYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis = patients[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ']]\n",
        "demographics = patients[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡∏≠‡∏≤‡∏¢‡∏∏(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û',\n",
        "                         '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']].drop_duplicates()\n",
        "\n",
        "df_model = pd.merge(df_analysis, demographics, on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n"
      ],
      "metadata": {
        "id": "ffUGfVqbnoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model.to_excel('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°1.xlsx', index=False)"
      ],
      "metadata": {
        "id": "VSd_efOnnq-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n"
      ],
      "metadata": {
        "id": "WgiDx3VggjCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uAxrEwxYpGaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"
      ],
      "metadata": {
        "id": "UJOg1f07g-YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df = pd.read_excel('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°1.xlsx')\n",
        "\n",
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£\n",
        "cols = ['‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "df = df[cols].copy()\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ (‡∏ô‡∏±‡∏ö‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°)\n",
        "df['‡∏ô‡∏±‡∏ö'] = 1\n",
        "grouped = df.groupby(cols).agg({'‡∏ô‡∏±‡∏ö': 'sum'}).reset_index()\n",
        "X = grouped[cols]\n",
        "y = grouped['‡∏ô‡∏±‡∏ö']\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó category ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "X_encoded = X.copy()\n",
        "le_dict = {}\n",
        "for col in X_encoded.select_dtypes(include='object'):\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
        "    le_dict[col] = le\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "\n",
        "# üî∏ Random Forest\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "rf_rmse = np.sqrt(rf_mse)\n",
        "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
        "\n",
        "# üî∏ Deep Learning\n",
        "dl_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "dl_model.compile(optimizer='adam', loss='mse')\n",
        "dl_model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "dl_pred = dl_model.predict(X_test).flatten()\n",
        "mse = mean_squared_error(y_test, dl_pred)\n",
        "dl_rmse = np.sqrt(mse)\n",
        "\n",
        "dl_mae = mean_absolute_error(y_test, dl_pred)\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "print(\"üîπ Random Forest:\")\n",
        "print(\"  RMSE:\", rf_rmse)\n",
        "print(\"  MAE :\", rf_mae)\n",
        "\n",
        "print(\"\\nüîπ Deep Learning:\")\n",
        "print(\"  RMSE:\", dl_rmse)\n",
        "print(\"  MAE :\", dl_mae)\n"
      ],
      "metadata": {
        "id": "OWp2_OAChDmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import xgboost as xgb\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df = pd.read_excel('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°1.xlsx')\n",
        "\n",
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£\n",
        "cols = ['‡πÄ‡∏û‡∏®', '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ï‡∏≥‡∏ö‡∏•', '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']\n",
        "df = df[cols].copy()\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ (‡∏ô‡∏±‡∏ö‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°)\n",
        "df['‡∏ô‡∏±‡∏ö'] = 1\n",
        "grouped = df.groupby(cols).agg({'‡∏ô‡∏±‡∏ö': 'sum'}).reset_index()\n",
        "X = grouped[cols]\n",
        "y = grouped['‡∏ô‡∏±‡∏ö']\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó category ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "X_encoded = X.copy()\n",
        "le_dict = {}\n",
        "for col in X_encoded.select_dtypes(include='object'):\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
        "    le_dict[col] = le\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Deep Learning\n",
        "dl_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "dl_model.compile(optimizer='adam', loss='mse')\n",
        "dl_model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "dl_pred = dl_model.predict(X_test).flatten()\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "def print_metrics(y_true, y_pred, model_name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"üîπ {model_name}:\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\")\n",
        "    print(f\"  MAE : {mae:.4f}\")\n",
        "    print(f\"  R2  : {r2:.4f}\\n\")\n",
        "\n",
        "print_metrics(y_test, rf_pred, \"Random Forest\")\n",
        "print_metrics(y_test, dl_pred, \"Deep Learning\")\n",
        "print_metrics(y_test, xgb_pred, \"XGBoost\")\n"
      ],
      "metadata": {
        "id": "taPkm2QnYA90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = X_test.copy()\n",
        "results_df['Actual'] = y_test.values\n",
        "results_df['RF_Pred'] = rf_pred\n",
        "results_df['DL_Pred'] = dl_pred\n",
        "results_df['XGB_Pred'] = xgb_pred\n",
        "\n",
        "results_df.to_excel('‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•.xlsx', index=False)\n",
        "print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•1.xlsx\")\n"
      ],
      "metadata": {
        "id": "_Dc0AZgjYeZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‡∏ó‡∏≥‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"
      ],
      "metadata": {
        "id": "gj0ONhabiCDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°1.xlsx')\n",
        "print(df.columns)  # ‡∏î‡∏π‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏£‡∏¥‡∏á\n"
      ],
      "metadata": {
        "id": "SrW9J5Ollhap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df = pd.read_excel('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°1.xlsx')\n",
        "\n",
        "# 2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)\n",
        "df = df.rename(columns={\n",
        "    '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢': 'count_patients',\n",
        "    '‡πÄ‡∏û‡∏®': 'gender',\n",
        "    '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)': 'age_year',\n",
        "    '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û': 'occupation',\n",
        "    '‡∏ï‡∏≥‡∏ö‡∏•': 'subdistrict',\n",
        "    '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠': 'district',\n",
        "    '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': 'month',\n",
        "    '‡∏õ‡∏µ': 'year',\n",
        "    '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™': 'quarter',\n",
        "    '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•': 'season'\n",
        "})\n",
        "\n",
        "# 3. ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°\n",
        "df_count = df.groupby([\n",
        "    'gender', 'age_year', 'occupation', 'subdistrict', 'district',\n",
        "    'month', 'year', 'quarter', 'season'\n",
        "]).size().reset_index(name='count_patients')\n",
        "\n",
        "# 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏ï‡∏£ (formula) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö glm\n",
        "formula = 'count_patients ~ gender + age_year + occupation + subdistrict + district + month + year + quarter + season'\n",
        "\n",
        "# 5. Fit Poisson Regression\n",
        "poisson_model = smf.glm(formula=formula, data=df_count, family=sm.families.Poisson()).fit()\n",
        "\n",
        "# 6. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "print(poisson_model.summary())\n",
        "\n",
        "# 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö overdispersion\n",
        "dispersion = poisson_model.deviance / poisson_model.df_resid\n",
        "print(f'\\nDispersion ratio: {dispersion:.2f}')\n",
        "\n",
        "if dispersion > 1.5:\n",
        "    print(\"\\nOverdispersion detected! Fit Negative Binomial Regression instead.\\n\")\n",
        "    nb_model = smf.glm(formula=formula, data=df_count, family=sm.families.NegativeBinomial()).fit()\n",
        "    print(nb_model.summary())\n",
        "else:\n",
        "    print(\"\\nNo significant overdispersion detected. Poisson model is fine.\")\n"
      ],
      "metadata": {
        "id": "lENeOOTzmIrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "significant_vars = poisson_model.pvalues[poisson_model.pvalues < 0.05].index.tolist()\n",
        "print(\"‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (p-value < 0.05):\")\n",
        "print(significant_vars)\n"
      ],
      "metadata": {
        "id": "d03I3fiZoHad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "significant_vars = ['occupation', 'subdistrict', 'district', 'season', 'month', 'year', 'quarter']\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á categorical variables ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô C(variable)\n",
        "formula_terms = []\n",
        "for var in significant_vars:\n",
        "    if var in ['occupation', 'subdistrict', 'district', 'season']:\n",
        "        formula_terms.append(f'C({var})')\n",
        "    else:\n",
        "        formula_terms.append(var)\n",
        "\n",
        "formula_significant = 'count_patients ~ ' + ' + '.join(formula_terms)\n",
        "\n",
        "poisson_model_sig = smf.glm(formula=formula_significant, data=df_count, family=sm.families.Poisson()).fit()\n",
        "\n",
        "print(poisson_model_sig.summary())\n"
      ],
      "metadata": {
        "id": "80oVQtqJoTls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_int = poisson_model_sig.conf_int()\n",
        "results_df = pd.DataFrame({\n",
        "    'coef': params,\n",
        "    'p_value': pvalues,\n",
        "    'conf_low': conf_int[0],\n",
        "    'conf_high': conf_int[1]\n",
        "})\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
        "print(results_df[results_df['p_value'] < 0.05])\n"
      ],
      "metadata": {
        "id": "xDr9MW2Mot4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ‡∏£‡∏ß‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π\n",
        "df_season = df.groupby('season').size().reset_index(name='count_patients')\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df_season, x='season', y='count_patients', palette='coolwarm')\n",
        "plt.title('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÑ‡∏Ç‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π')\n",
        "plt.xlabel('‡∏§‡∏î‡∏π')\n",
        "plt.ylabel('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WRK3MYkbpJP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48XVFHM_VKBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}