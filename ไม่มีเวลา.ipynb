{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nL_UEuciHQLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ó\n",
        "file_path = 'rain.xlsx'\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "df_list = []\n",
        "for sheet in xls.sheet_names:\n",
        "    df = pd.read_excel(xls, sheet_name=sheet)\n",
        "    df = df.rename(columns={'‡∏õ‡∏µ': 'year', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': 'month', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': 'day'})  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
        "\n",
        "    #  ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á\n",
        "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "    df['month'] = pd.to_numeric(df['month'], errors='coerce')\n",
        "    df['day'] = pd.to_numeric(df['day'], errors='coerce')\n",
        "    df = df.dropna(subset=['year', 'month', 'day'])\n",
        "\n",
        "    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
        "    df['‡∏ß‡∏±‡∏ô'] = pd.to_datetime(df[['year', 'month', 'day']], errors='coerce')\n",
        "    df = df.dropna(subset=['‡∏ß‡∏±‡∏ô'])\n",
        "\n",
        "    #  ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥/‡∏ù‡∏ô/‡∏ä‡∏∑‡πâ‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0 ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö)\n",
        "    for col in ['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    #  ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ NaN ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å (‡πÅ‡∏ï‡πà 0 ‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà)\n",
        "    df = df[df[['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']].notna().all(axis=1)]\n",
        "\n",
        "    df_list.append(df)\n",
        "\n",
        "#  ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏ä‡∏µ‡∏ó‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
        "weather = pd.concat(df_list, ignore_index=True)\n",
        "weather = weather.drop_duplicates()\n",
        "weather = weather.sort_values('‡∏ß‡∏±‡∏ô').reset_index(drop=True)\n",
        "\n",
        "#  ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß\n",
        "weather.to_excel('rain_all_years_sorted.xlsx', index=False)\n",
        "print(\" ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå rain_all_years_sorted.xlsx ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n",
        "#  ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢\n",
        "xls = pd.ExcelFile('DHFcc12.xlsx')\n",
        "df_list = []\n",
        "\n",
        "for sheet in xls.sheet_names:\n",
        "    df = pd.read_excel(xls, sheet_name=sheet)\n",
        "    df.columns = df.columns.str.strip()  # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
        "\n",
        "    #  ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô datetime\n",
        "    if '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢' in df.columns:\n",
        "        df['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "\n",
        "    #  ‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0 ‡πÑ‡∏î‡πâ)\n",
        "    for col in ['‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)', '‡∏≠‡∏≤‡∏¢‡∏∏(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df_list.append(df)\n",
        "\n",
        "#  ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢\n",
        "patients = pd.concat(df_list, ignore_index=True)\n",
        "patients = patients.drop_duplicates()\n",
        "patients = patients.dropna(subset=['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "\n",
        "print(f\" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢: {len(patients)}\")\n",
        "\n",
        "#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì rolling average\n",
        "def get_rolling_avg(date, n_days=15):\n",
        "    start_date = date - pd.Timedelta(days=n_days)\n",
        "    mask = (weather['‡∏ß‡∏±‡∏ô'] >= start_date) & (weather['‡∏ß‡∏±‡∏ô'] < date)\n",
        "    subset = weather.loc[mask]\n",
        "    if subset.empty:\n",
        "        return pd.Series([None, None, None])\n",
        "    return pd.Series([\n",
        "        subset['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥'].mean(),\n",
        "        subset['‡∏ù‡∏ô'].mean(),\n",
        "        subset['‡∏ä‡∏∑‡πâ‡∏ô'].mean()  #‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà date ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ ‡∏ù‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á n_days ‡∏ß‡∏±‡∏ô\n",
        "    ])\n",
        "\n",
        "#  ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 15 ‡∏ß‡∏±‡∏ô\n",
        "patients[['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg']] = patients['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].apply(get_rolling_avg)\n",
        "\n",
        "print(\" ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ temp, rain ‡πÅ‡∏•‡∏∞ humid ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 15 ‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡∏õ‡∏µ ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™ ‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•\n",
        "patients['‡∏õ‡∏µ'] = patients['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "patients['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = patients['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "patients['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = patients['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "\n",
        "#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏§‡∏î‡∏π‡πÅ‡∏ö‡∏ö‡πÑ‡∏ó‡∏¢\n",
        "def assign_season(row):\n",
        "    month = row['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']\n",
        "    day = row['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].day\n",
        "\n",
        "    if (month == 2 and day >= 15) or month in [3, 4] or (month == 5 and day < 15):\n",
        "        return 'Summer'\n",
        "    elif (month == 5 and day >= 15) or month in [6, 7, 8, 9] or (month == 10 and day < 15):\n",
        "        return 'Rainy'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "patients['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = patients.apply(assign_season, axis=1)\n",
        "\n",
        "#  ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô\n",
        "if '‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ' in patients.columns:\n",
        "    daily_counts = patients.groupby('‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "\n",
        "    daily_weather = patients.groupby('‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ').agg({\n",
        "        'temp_15d_avg': 'mean',\n",
        "        'rain_15d_avg': 'mean',\n",
        "        'humid_15d_avg': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    df_analysis = pd.merge(daily_counts, daily_weather, on='‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ', how='left')\n",
        "\n",
        "    daily_vars = patients.groupby('‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ').agg({\n",
        "        '‡∏≠‡∏≤‡∏¢‡∏∏(‡∏õ‡∏µ)': 'mean',\n",
        "        '‡∏≠‡∏≤‡∏¢‡∏∏(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)': 'mean',\n",
        "        '‡πÄ‡∏û‡∏®': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏ï‡∏≥‡∏ö‡∏•': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏õ‡∏µ': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
        "        '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•': lambda x: x.mode()[0] if not x.mode().empty else None\n",
        "    }).reset_index()\n",
        "\n",
        "    df_analysis = pd.merge(df_analysis, daily_vars, on='‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ', how='left')\n",
        "    print(\"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ö‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏ß‡∏±‡∏ô‡∏û‡∏ö‡∏ú‡∏õ' ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢\")\n"
      ],
      "metadata": {
        "id": "KCt9r8gpUic8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ 0 ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ===\")\n",
        "\n",
        "for col in df_analysis.columns:\n",
        "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN\n",
        "    series = df_analysis[col].dropna()\n",
        "\n",
        "    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö 0 ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
        "    try:\n",
        "        n_zeros = (series == 0).sum()\n",
        "        perc_zeros = n_zeros / len(series) * 100\n",
        "        print(f\"{col}: 0 ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô = {n_zeros}, 0 ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå = {perc_zeros:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"{col}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ 0 ‡πÑ‡∏î‡πâ ({str(e)})\")\n"
      ],
      "metadata": {
        "id": "ZVTa9yb9KCpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(daily_vars.head())\n"
      ],
      "metadata": {
        "id": "T9JuClh77wQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Correlation / Chi-Square)"
      ],
      "metadata": {
        "id": "qTxpJUaSH1vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis.rename(columns={'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢': 'cases'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "SttWo-mMb2As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis.rename(columns={\n",
        "    'temp_15d_avg': 'temp',\n",
        "    'rain_15d_avg': 'rain',\n",
        "    'humid_15d_avg': 'humidity',\n",
        "    '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô': 'village',\n",
        "    '‡∏ï‡∏≥‡∏ö‡∏•': 'sub_district',\n",
        "    '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠': 'district',\n",
        "    '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': 'month',\n",
        "    '‡∏õ‡∏µ': 'year',\n",
        "    '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™': 'quarter',\n",
        "    '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•': 'season'\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "id": "D0YN4uHQcKlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ ---\n",
        "numeric_vars = ['temp', 'rain', 'humidity']\n",
        "categorical_vars = ['village', 'sub_district', 'district', 'month', 'year', 'quarter', 'season']\n",
        "\n",
        "# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÅ‡∏ö‡∏ö‡πÅ‡∏ö‡πà‡∏á 3 ‡∏Å‡∏•‡∏∏‡πà‡∏° ---\n",
        "df_analysis['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏Å‡∏•‡∏∏‡πà‡∏°'] = pd.qcut(df_analysis['cases'], q=3, labels=False, duplicates='drop')\n",
        "\n",
        "# === Correlation ===\n",
        "print(\"=== üìä Numeric Variables Correlation with ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ===\")\n",
        "for var in numeric_vars:\n",
        "    corr = df_analysis[var].corr(df_analysis['cases'])\n",
        "    print(f\"{var} vs ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢: correlation = {corr:.4f}\")\n",
        "\n",
        "# === Chi-square test ===\n",
        "print(\"\\n=== üß™ Categorical Variables Association with ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ (Chi-square test) ===\")\n",
        "for var in categorical_vars:\n",
        "    table = pd.crosstab(df_analysis[var], df_analysis['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏Å‡∏•‡∏∏‡πà‡∏°'])\n",
        "    if table.shape[0] < 2 or table.shape[1] < 2:\n",
        "        print(f\"{var} ‚Äì ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chi-square\")\n",
        "        continue\n",
        "    chi2, p, dof, ex = chi2_contingency(table)\n",
        "    print(f\"{var} vs ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ (‡∏Å‡∏•‡∏∏‡πà‡∏°): p-value = {p:.4f}\")\n",
        "\n",
        "# === ‡πÄ‡∏ä‡πá‡∏Ñ Poisson dispersion ===\n",
        "mean_count = df_analysis['cases'].mean()\n",
        "var_count = df_analysis['cases'].var()\n",
        "dispersion_ratio = var_count / mean_count\n",
        "\n",
        "print(\"\\n=== üßÆ Poisson dispersion check ===\")\n",
        "print(f\"Mean ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ = {mean_count:.4f}\")\n",
        "print(f\"Variance ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ = {var_count:.4f}\")\n",
        "print(f\"Dispersion ratio (variance/mean) = {dispersion_ratio:.4f}\")\n",
        "\n",
        "if dispersion_ratio > 1.5:\n",
        "    print(\"‡∏°‡∏µ overdispersion (variance > mean) ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ä‡πâ Negative Binomial model\")\n",
        "else:\n",
        "    print(\"‡πÑ‡∏°‡πà‡∏°‡∏µ overdispersion ‡πÇ‡∏°‡πÄ‡∏î‡∏• Poisson ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\")\n"
      ],
      "metadata": {
        "id": "5K4p2cxO6Cyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_analysis.columns.tolist())\n"
      ],
      "metadata": {
        "id": "nXCjkDJkLokh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Modeling (Poisson ‡πÅ‡∏•‡∏∞ NB)"
      ],
      "metadata": {
        "id": "FQrXl_m4H87X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# ‡∏™‡∏π‡∏ï‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ\n",
        "formula = 'cases ~ temp + rain + humidity + C(village) + C(sub_district) + C(district) + C(month) + C(year) + C(quarter) + C(season)'\n",
        "\n",
        "# Fit Negative Binomial model\n",
        "negbin_model = smf.glm(formula=formula, data=df_analysis, family=sm.families.NegativeBinomial()).fit()\n",
        "\n",
        "# ‡∏î‡∏π summary ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• Negative Binomial\n",
        "print(negbin_model.summary())\n"
      ],
      "metadata": {
        "id": "5-MSYg6YH-nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_analysis.dtypes)\n",
        "print(df_analysis[['season', 'temp', 'rain', 'humidity']].isna().sum())\n"
      ],
      "metadata": {
        "id": "ED5yr7AQmWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis['village'] = df_analysis['village'].astype('category')\n",
        "df_analysis['district'] = df_analysis['district'].astype('category')\n"
      ],
      "metadata": {
        "id": "DqkdYC51nrjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "formula = 'cases ~ temp + rain + humidity + C(village) + C(sub_district) + C(district) + C(year) + C(season)'\n",
        "\n",
        "negbin_model = smf.glm(formula=formula, data=df_analysis, family=sm.families.NegativeBinomial()).fit()\n",
        "\n",
        "print(negbin_model.summary())\n"
      ],
      "metadata": {
        "id": "9ThGzau1UUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_analysis.dtypes)\n"
      ],
      "metadata": {
        "id": "buXj1KrKoI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_analysis['season'].cat.categories)\n",
        "print(df_analysis['village'].cat.categories)\n",
        "print(df_analysis['sub_district'].cat.categories)\n",
        "print(df_analysis['district'].cat.categories)\n"
      ],
      "metadata": {
        "id": "-I5g4e78oYqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_analysis.isna().sum())\n"
      ],
      "metadata": {
        "id": "JvRijND8oaeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"
      ],
      "metadata": {
        "id": "o8XR9D9tIK5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'coef': negbin_model.params,\n",
        "    'std_err': negbin_model.bse,\n",
        "    'z': negbin_model.tvalues,\n",
        "    'p_value': negbin_model.pvalues,\n",
        "    'conf_low': negbin_model.conf_int().iloc[:, 0],\n",
        "    'conf_high': negbin_model.conf_int().iloc[:, 1]\n",
        "})\n",
        "\n",
        "significant_vars = results_df[results_df['p_value'] < 0.05]\n",
        "\n",
        "print(\"=== ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (p < 0.05) ===\")\n",
        "print(significant_vars)\n"
      ],
      "metadata": {
        "id": "Vdd3gKSOUS7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning ‡∏î‡πâ‡∏ß‡∏¢ XGBoost & Random Forest"
      ],
      "metadata": {
        "id": "UBkeiZD3IQLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib\n"
      ],
      "metadata": {
        "id": "lX1Q-ZfjTY4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ dummy variables ‡∏Ç‡∏≠‡∏á year 2023 ‡πÅ‡∏•‡∏∞ 2024)\n",
        "selected_features = ['year']  # ‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏≠‡∏≤ 'year' ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á categorical 'year' ‡πÄ‡∏õ‡πá‡∏ô dummy variable\n",
        "X = pd.get_dummies(df_analysis[selected_features], drop_first=True)\n",
        "\n",
        "# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢\n",
        "y = df_analysis['cases']\n",
        "\n",
        "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Scaling ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
        "print(\" R2  :\", r2_score(y_test, y_pred_xgb))\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\nRandom Forest\")\n",
        "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))\n",
        "print(\" R2  :\", r2_score(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "dl_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "dl_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "dl_model.fit(X_train_scaled, y_train,\n",
        "             validation_split=0.2,\n",
        "             epochs=100,\n",
        "             batch_size=16,\n",
        "             verbose=0,\n",
        "             callbacks=[early_stop])\n",
        "\n",
        "y_pred_dl = dl_model.predict(X_test_scaled).flatten()\n",
        "\n",
        "print(\"\\nDeep Learning (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)\")\n",
        "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_dl)))\n",
        "print(\" R2  :\", r2_score(y_test, y_pred_dl))\n"
      ],
      "metadata": {
        "id": "FVwZAvRTTY7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "models = ['XGBoost', 'Random Forest', 'Deep Learning']\n",
        "rmse = [2.8410, 2.8401, 2.8416]\n",
        "r2 = [0.1859, 0.1864, 0.18560]\n",
        "\n",
        "x = np.arange(len(models))  # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏Å‡∏ô x\n",
        "width = 0.35  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# ‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü RMSE\n",
        "bars1 = ax.bar(x - width/2, rmse, width, label='RMSE', color='skyblue')\n",
        "\n",
        "# ‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü R2\n",
        "bars2 = ax.bar(x + width/2, r2, width, label='R2', color='orange')\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü\n",
        "def autolabel(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 3 ‡∏à‡∏∏‡∏î\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "autolabel(bars1)\n",
        "autolabel(bars2)\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏Å‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≤‡∏ü\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# ‡πÉ‡∏™‡πà‡∏Å‡∏£‡∏¥‡∏î‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏ï‡πâ‡∏Å‡∏£‡∏≤‡∏ü\n",
        "plt.figtext(0.5, -0.05, 'RMSE: Root Mean Squared Error (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ), R2: Coefficient of Determination (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QjyrhTbLrq4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "\n",
        "print(\"df_base columns:\", df_base.columns.tolist())\n",
        "print(\"df_weather columns:\", df_weather.columns.tolist())\n"
      ],
      "metadata": {
        "id": "86Ud5z1KTZBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_full.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "HFl3G64-RT8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß**\n"
      ],
      "metadata": {
        "id": "YOq04TcqaPHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
        "df_base = pd.read_excel('DHFcc12.xlsx')\n",
        "df_weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô datetime\n",
        "df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "df_weather['‡∏ß‡∏±‡∏ô'] = pd.to_datetime(df_weather['‡∏ß‡∏±‡∏ô'])\n",
        "df_weather.rename(columns={'‡∏ß‡∏±‡∏ô': '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'}, inplace=True)\n",
        "\n",
        "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô\n",
        "df_count = df_base.groupby('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 15 ‡∏ß‡∏±‡∏ô\n",
        "weather_agg = []\n",
        "for date in df_count['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢']:\n",
        "    mask = (df_weather['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'] >= date - pd.Timedelta(days=15)) & (df_weather['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'] < date)\n",
        "    past_weather = df_weather[mask]\n",
        "    weather_agg.append({\n",
        "        '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢': date,\n",
        "        '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥': past_weather['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥'].mean(),\n",
        "        '‡∏ù‡∏ô': past_weather['‡∏ù‡∏ô'].mean(),\n",
        "        '‡∏ä‡∏∑‡πâ‡∏ô': past_weather['‡∏ä‡∏∑‡πâ‡∏ô'].mean()\n",
        "    })\n",
        "\n",
        "df_weather_agg = pd.DataFrame(weather_agg)\n",
        "\n",
        "# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 15 ‡∏ß‡∏±‡∏ô\n",
        "df_full = pd.merge(df_count, df_weather_agg, on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "df_full['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "df_full['‡∏õ‡∏µ'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "df_full['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢\n",
        "def assign_season(row):\n",
        "    month = row['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']\n",
        "    day = row['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].day\n",
        "    if (month == 2 and day >= 15) or month in [3, 4] or (month == 5 and day < 15):\n",
        "        return 'Summer'\n",
        "    elif (month == 5 and day >= 15) or month in [6, 7, 8, 9] or (month == 10 and day < 15):\n",
        "        return 'Rainy'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "df_full['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = df_full.apply(assign_season, axis=1)\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á lag features (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡πà‡∏≠‡∏ô)\n",
        "df_full = df_full.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(1)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(7)\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN (‡∏à‡∏≤‡∏Å lag)\n",
        "df_full.dropna(inplace=True)\n",
        "\n",
        "# Reset index ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ train_test_split ‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏ö‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÜ\n",
        "df_full = df_full.reset_index(drop=True)\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∏‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "feature_sets = {\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "}\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, features in feature_sets.items():\n",
        "    df = df_full[features + ['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']].copy()\n",
        "\n",
        "    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡πÉ‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå ‡πÉ‡∏´‡πâ‡∏ó‡∏≥ One-Hot Encoding\n",
        "    if '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•' in features:\n",
        "        season_encoded = ohe.fit_transform(df[['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']])\n",
        "        season_cols = [f\"‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•_{cat}\" for cat in ohe.categories_[0][1:]]\n",
        "        df = pd.concat([df.drop(columns=['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']), pd.DataFrame(season_encoded, columns=season_cols, index=df.index)], axis=1)\n",
        "    else:\n",
        "        season_cols = []\n",
        "\n",
        "    X = df.drop(columns=['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "    y = df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']\n",
        "\n",
        "    # Scale ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏• (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡πÄ‡∏õ‡πá‡∏ô one-hot ‡πÅ‡∏•‡πâ‡∏ß)\n",
        "    num_cols = [col for col in X.columns if col not in season_cols]\n",
        "    scaler = StandardScaler()\n",
        "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    # ‡πÅ‡∏ö‡πà‡∏á train/test ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest\n",
        "    model = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results.append({'‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.sort_values('R2', ascending=False).reset_index(drop=True))\n"
      ],
      "metadata": {
        "id": "3QLvIZ_r7VqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full = df_full.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag14'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(14)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag30'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(30)\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ shift\n",
        "df_full = df_full.dropna().reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "sMTWo0SEeT9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XG-BOOST"
      ],
      "metadata": {
        "id": "CDHW2mfLerPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
        "df_base = pd.read_excel('DHFcc12.xlsx')\n",
        "df_weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "\n",
        "# --- ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô datetime ---\n",
        "df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "df_weather['‡∏ß‡∏±‡∏ô'] = pd.to_datetime(df_weather['‡∏ß‡∏±‡∏ô'])\n",
        "df_weather.rename(columns={'‡∏ß‡∏±‡∏ô': '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'}, inplace=True)\n",
        "\n",
        "# --- ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô ---\n",
        "df_count = df_base.groupby('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "\n",
        "# --- **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 15 ‡∏ß‡∏±‡∏ô** ---\n",
        "# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ï‡∏£‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏•‡∏¢ (merge ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà)\n",
        "df_full = pd.merge(df_count, df_weather[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']], on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n",
        "\n",
        "# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ---\n",
        "df_full['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "df_full['‡∏õ‡∏µ'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "df_full['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "\n",
        "# --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏• ---\n",
        "def assign_season(row):\n",
        "    month = row['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']\n",
        "    day = row['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].day\n",
        "    if (month == 2 and day >= 15) or month in [3, 4] or (month == 5 and day < 15):\n",
        "        return 'Summer'\n",
        "    elif (month == 5 and day >= 15) or month in [6, 7, 8, 9] or (month == 10 and day < 15):\n",
        "        return 'Rainy'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "df_full['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = df_full.apply(assign_season, axis=1)\n",
        "\n",
        "# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á lag features ---\n",
        "df_full = df_full.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(1)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(7)\n",
        "\n",
        "# --- ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ---\n",
        "df_full.dropna(inplace=True)\n",
        "df_full = df_full.reset_index(drop=True)  # reset index ‡∏´‡∏•‡∏±‡∏á dropna\n",
        "\n",
        "# --- ‡∏ä‡∏∏‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå ---\n",
        "feature_sets = {\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "}\n",
        "\n",
        "# --- One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏• ---\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, features in feature_sets.items():\n",
        "    df = df_full[features + ['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']].copy()\n",
        "\n",
        "    if '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•' in features:\n",
        "        season_encoded = ohe.fit_transform(df[['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']])\n",
        "        season_cols = [f\"‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•_{cat}\" for cat in ohe.categories_[0][1:]]\n",
        "        df = pd.concat([df.drop(columns=['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']), pd.DataFrame(season_encoded, columns=season_cols, index=df.index)], axis=1)\n",
        "    else:\n",
        "        season_cols = []\n",
        "\n",
        "    X = df.drop(columns=['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "    y = df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in season_cols]\n",
        "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results.append({'‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.sort_values('R2', ascending=False).reset_index(drop=True))\n"
      ],
      "metadata": {
        "id": "DrA07O3tpQfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning"
      ],
      "metadata": {
        "id": "IqLRqvPPjHCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á random seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° reproducible\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
        "df_base = pd.read_excel('DHFcc12.xlsx')\n",
        "df_weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô datetime\n",
        "df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "df_weather['‡∏ß‡∏±‡∏ô'] = pd.to_datetime(df_weather['‡∏ß‡∏±‡∏ô'])\n",
        "\n",
        "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô df_weather ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö df_base ‡πÄ‡∏û‡∏∑‡πà‡∏≠ merge\n",
        "df_weather.rename(columns={'‡∏ß‡∏±‡∏ô': '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'}, inplace=True)\n",
        "\n",
        "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô\n",
        "df_count = df_base.groupby('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "\n",
        "# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)\n",
        "df_full = pd.merge(df_count, df_weather[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']], on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏Å‡∏≤‡∏®\n",
        "df_full.dropna(subset=['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô'], inplace=True)\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "df_full['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "df_full['‡∏õ‡∏µ'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "df_full['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡πÑ‡∏ó‡∏¢\n",
        "def assign_season(row):\n",
        "    month = row['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']\n",
        "    day = row['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].day\n",
        "    if (month == 2 and day >= 15) or month in [3, 4] or (month == 5 and day < 15):\n",
        "        return 'Summer'\n",
        "    elif (month == 5 and day >= 15) or month in [6, 7, 8, 9] or (month == 10 and day < 15):\n",
        "        return 'Rainy'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "df_full['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = df_full.apply(assign_season, axis=1)\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á lag features\n",
        "df_full = df_full.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(1)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(7)\n",
        "\n",
        "# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN ‡∏à‡∏≤‡∏Å lag features\n",
        "df_full.dropna(inplace=True)\n",
        "df_full = df_full.reset_index(drop=True)  # reset index ‡∏´‡∏•‡∏±‡∏á dropna\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∏‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå (‡πÄ‡∏û‡∏¥‡πà‡∏° '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô' ‡∏î‡πâ‡∏ß‡∏¢)\n",
        "feature_sets = {\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏ù‡∏ô\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "    \"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\": ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'],\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
        "\n",
        "for name, features in feature_sets.items():\n",
        "    df = df_full[features + ['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']].copy()\n",
        "\n",
        "    # One-hot encoding ‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏• ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô features\n",
        "    if '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•' in features:\n",
        "        season_encoded = ohe.fit_transform(df[['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']])\n",
        "        season_cols = [f\"‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•_{cat}\" for cat in ohe.categories_[0][1:]]\n",
        "        df = pd.concat([df.drop(columns=['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']), pd.DataFrame(season_encoded, columns=season_cols, index=df.index)], axis=1)\n",
        "    else:\n",
        "        season_cols = []\n",
        "\n",
        "    X = df.drop(columns=['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "    y = df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].values\n",
        "\n",
        "    # Scale ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° one-hot)\n",
        "    scaler = StandardScaler()\n",
        "    num_cols = [col for col in X.columns if col not in season_cols]\n",
        "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    X = X.values\n",
        "\n",
        "    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train-test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0,\n",
        "              validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results.append({'‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values('R2', ascending=False).reset_index(drop=True)\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "2K12Yxz4p-Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df_base = pd.read_excel('DHFcc12.xlsx')\n",
        "df_weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "\n",
        "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "df_weather['‡∏ß‡∏±‡∏ô'] = pd.to_datetime(df_weather['‡∏ß‡∏±‡∏ô'])\n",
        "df_weather.rename(columns={'‡∏ß‡∏±‡∏ô': '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'}, inplace=True)\n",
        "\n",
        "# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "df_count = df_base.groupby('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "df_full = pd.merge(df_count, df_weather[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']], on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n",
        "df_full.dropna(subset=['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô'], inplace=True)\n",
        "\n",
        "# ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "df_full['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "df_full['‡∏õ‡∏µ'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "df_full['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = df_full['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "df_full['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = df_full['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'].apply(lambda m: 'Summer' if m in [3,4,5] else 'Rainy' if m in [6,7,8,9] else 'Winter')\n",
        "\n",
        "# lag features\n",
        "df_full = df_full.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(1)\n",
        "df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'] = df_full['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(7)\n",
        "df_full.dropna(inplace=True)\n",
        "\n",
        "# ‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ\n",
        "features = ['‡∏õ‡∏µ', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7']\n",
        "df = df_full[features + ['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']].copy()\n",
        "\n",
        "# One-hot encoding ‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
        "season_encoded = ohe.fit_transform(df[['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']])\n",
        "season_cols = [f\"‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•_{cat}\" for cat in ohe.categories_[0][1:]]\n",
        "df = pd.concat([df.drop(columns=['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']), pd.DataFrame(season_encoded, columns=season_cols, index=df.index)], axis=1)\n",
        "\n",
        "# ‡πÅ‡∏¢‡∏Å X y\n",
        "X = df.drop(columns=['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "y = df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].values\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "num_cols = [col for col in X.columns if col not in season_cols]\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "X = X.values\n",
        "\n",
        "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ‡πÄ‡∏ó‡∏£‡∏ô XGBoost\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=4, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
        "print(\"üìä ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ + ‡∏ù‡∏ô\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE : {mae:.4f}\")\n",
        "print(f\"R2  : {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "oGAGegl04JQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1XjFG27q4JTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏® ---\n",
        "weather = pd.read_excel('rain_all_years_sorted.xlsx')\n",
        "weather.rename(columns={'‡∏ß‡∏±‡∏ô': '‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'}, inplace=True)\n",
        "\n",
        "# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ---\n",
        "df_base = pd.read_excel('DHFcc12.xlsx')\n",
        "df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] = pd.to_datetime(df_base['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'], errors='coerce')\n",
        "df_base = df_base.dropna(subset=['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "\n",
        "# --- ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ---\n",
        "df_count = df_base.groupby('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').size().reset_index(name='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢')\n",
        "\n",
        "# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô rolling average ---\n",
        "def get_rolling_avg(date, n_days=15):\n",
        "    start_date = date - pd.Timedelta(days=n_days)\n",
        "    mask = (weather['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] >= start_date) & (weather['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'] < date)\n",
        "    subset = weather.loc[mask]\n",
        "    if subset.empty:\n",
        "        return pd.Series([np.nan, np.nan, np.nan])\n",
        "    return pd.Series([\n",
        "        subset['‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥'].mean(),\n",
        "        subset['‡∏ù‡∏ô'].mean(),\n",
        "        subset['‡∏ä‡∏∑‡πâ‡∏ô'].mean()\n",
        "    ])\n",
        "\n",
        "df_count[['temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg']] = df_count['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].apply(get_rolling_avg)\n",
        "\n",
        "# --- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏á‡∏ß‡∏±‡∏ô ---\n",
        "df_count = pd.merge(df_count, weather[['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô']], on='‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢', how='left')\n",
        "\n",
        "# --- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏ß‡∏•‡∏≤ ---\n",
        "df_count['‡∏õ‡∏µ'] = df_count['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.year\n",
        "df_count['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'] = df_count['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.month\n",
        "df_count['‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™'] = df_count['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].dt.quarter\n",
        "\n",
        "def assign_season(row):\n",
        "    month = row['‡πÄ‡∏î‡∏∑‡∏≠‡∏ô']\n",
        "    day = row['‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢'].day\n",
        "    if (month == 2 and day >= 15) or month in [3, 4] or (month == 5 and day < 15):\n",
        "        return 'Summer'\n",
        "    elif (month == 5 and day >= 15) or month in [6, 7, 8, 9] or (month == 10 and day < 15):\n",
        "        return 'Rainy'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "df_count['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•'] = df_count.apply(assign_season, axis=1)\n",
        "\n",
        "# --- ‡πÄ‡∏û‡∏¥‡πà‡∏° lag features ---\n",
        "df_count = df_count.sort_values('‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πà‡∏ß‡∏¢').reset_index(drop=True)\n",
        "df_count['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1'] = df_count['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(1)\n",
        "df_count['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'] = df_count['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'].shift(7)\n",
        "\n",
        "# --- ‡∏•‡∏ö NaN ---\n",
        "df_count = df_count.dropna().reset_index(drop=True)\n",
        "\n",
        "# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô train/test ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ---\n",
        "def train_evaluate_model(feature_cols, df, model_name):\n",
        "    df_model = df[feature_cols + ['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']].copy()\n",
        "\n",
        "    if '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•' in feature_cols:\n",
        "        ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
        "        season_encoded = ohe.fit_transform(df_model[['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']])\n",
        "        season_cols = [f\"‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•_{cat}\" for cat in ohe.categories_[0][1:]]\n",
        "        df_model = pd.concat([df_model.drop(columns=['‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•']), pd.DataFrame(season_encoded, columns=season_cols, index=df_model.index)], axis=1)\n",
        "    else:\n",
        "        season_cols = []\n",
        "\n",
        "    X = df_model.drop(columns=['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢'])\n",
        "    y = df_model['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    num_cols = [col for col in X.columns if col not in season_cols]\n",
        "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'y_test': y_test.reset_index(drop=True),\n",
        "        'y_pred': pd.Series(y_pred)\n",
        "    }\n",
        "\n",
        "# --- ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö ---\n",
        "features_rolling = [\n",
        "    '‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•',\n",
        "    'temp_15d_avg', 'rain_15d_avg', 'humid_15d_avg',\n",
        "    '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'\n",
        "]\n",
        "\n",
        "features_direct = [\n",
        "    '‡∏õ‡∏µ', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™', '‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•',\n",
        "    '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡∏ä‡∏∑‡πâ‡∏ô',\n",
        "    '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag1', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢_lag7'\n",
        "]\n",
        "\n",
        "# --- ‡∏£‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ---\n",
        "results = []\n",
        "results.append(train_evaluate_model(features_rolling, df_count, 'Rolling 15-day average features'))\n",
        "results.append(train_evaluate_model(features_direct, df_count, 'Direct daily features'))\n",
        "\n",
        "# --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ---\n",
        "df_results = pd.DataFrame([{\n",
        "    'Model': r['Model'],\n",
        "    'RMSE': r['RMSE'],\n",
        "    'MAE': r['MAE'],\n",
        "    'R2': r['R2']\n",
        "} for r in results])\n",
        "print(\"=== ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ===\")\n",
        "print(df_results)\n",
        "\n",
        "# --- ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ---\n",
        "for r in results:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.scatter(r['y_test'], r['y_pred'], alpha=0.5)\n",
        "    plt.plot([r['y_test'].min(), r['y_test'].max()], [r['y_test'].min(), r['y_test'].max()], 'r--')\n",
        "    plt.xlabel(\"‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (Actual)\")\n",
        "    plt.ylabel(\"‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Predicted)\")\n",
        "    plt.title(f\"{r['Model']} - ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "PKnCLRZk4JVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}